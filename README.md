
# ğŸ§  MiniLLM â€“ Tiny Transformer Chatbot from Scratch

This is a personal deep learning project where I built and trained a minimal character-level Transformer language model to generate chatbot responses. The model is decoder-only (like GPT) and trained on a small dialogue dataset using PyTorch.

---

## ğŸ’¡ What it does

- Implements a decoder-only Transformer (multi-head attention + positional embeddings)
- Tokenizes text character-by-character with a custom tokenizer
- Trains on a simple dialogue corpus between User and Bot
- Generates responses conditioned on user input using temperature sampling

---

## ğŸ“ˆ Project Status

ğŸš§ This project is **still in progress**. Several improvements are planned:

- ğŸ“Š Add a **larger and more diverse dataset** for training  
- âš¡ Allow **training on GPU** (currently CPU-only)
- ğŸ“¦ Add better sampling (e.g., top-k, nucleus sampling)
- ğŸ§  Improve model depth and regularization
- ğŸ“„ Save/load tokenizer alongside model

---

## â–¶ï¸ How to Run

1. Install dependencies:
```bash
pip install -r requirements.txt
```

2. Train the model:
```bash
python train.py
```
The model will be saved in `checkpoints/mini_gpt.pth`.

3. Run the chatbot:
```bash
python generate.py
```

You can then interact with the chatbot in your terminal.

---

## ğŸ“‚ Project Structure

```
mini-llm/
â”œâ”€â”€ dataset.txt        # Training data (User/Bot format)
â”œâ”€â”€ model.py           # Decoder-only Transformer implementation
â”œâ”€â”€ tokenizer.py       # Char-level tokenizer
â”œâ”€â”€ train.py           # Training script
â”œâ”€â”€ generate.py        # Inference script (chat interface)
â”œâ”€â”€ utils.py           # Model saving/loading utilities
â””â”€â”€ checkpoints/       # Saved model
```

---

## ğŸ“Œ Technologies Used

PyTorch, Python 3.10+, TransformerDecoder, Custom Tokenizer

---
